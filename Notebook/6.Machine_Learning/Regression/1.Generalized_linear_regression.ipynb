{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---+\n",
      "|age|education|wantsMore|  y|\n",
      "+---+---------+---------+---+\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "|<25|      low|      yes|  0|\n",
      "+---+---------+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cuse = spark.read.csv(\"cuse_binary.csv\",header=True,inferSchema=True)\n",
    "cuse.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'education', 'wantsMore']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuse.columns[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {Row(age='<25'): 397,\n",
       "             Row(age='25-29'): 404,\n",
       "             Row(age='30-39'): 612,\n",
       "             Row(age='40-49'): 194})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuse.select(\"age\").rdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|  age|indexed_age|\n",
      "+-----+-----------+\n",
      "|30-39|        0.0|\n",
      "|  <25|        2.0|\n",
      "|25-29|        1.0|\n",
      "|40-49|        3.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "indexers = [StringIndexer(inputCol=column,outputCol='indexed_'+column) for column in (\"age\",\"education\",\"wantsMore\")]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_cuse = pipeline.fit(cuse).transform(cuse)\n",
    "indexed_cuse.select(\"age\",\"indexed_age\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------------+---+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|  y|\n",
      "+----------------+----------------------+----------------------+---+\n",
      "|   (3,[1],[1.0])|             (1,[],[])|         (1,[0],[1.0])|  0|\n",
      "|   (3,[2],[1.0])|         (1,[0],[1.0])|             (1,[],[])|  1|\n",
      "|   (3,[0],[1.0])|         (1,[0],[1.0])|         (1,[0],[1.0])|  0|\n",
      "|       (3,[],[])|         (1,[0],[1.0])|         (1,[0],[1.0])|  1|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|  0|\n",
      "+----------------+----------------------+----------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "columns = indexed_cuse.columns[0:3]\n",
    "onehotencoders = [OneHotEncoder(inputCol='indexed_'+column,outputCol='onehotencode_'+column)for column in columns]\n",
    "pipeline = Pipeline(stages = onehotencoders)\n",
    "onehotencoder_columns = ['onehotencode_age','onehotencode_education','onehotencode_wantsMore','y']\n",
    "onehotencode_cuse = pipeline.fit(indexed_cuse).transform(indexed_cuse).select(onehotencoder_columns)\n",
    "onehotencode_cuse.distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------------+-----+-------------------+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label|           features|\n",
      "+----------------+----------------------+----------------------+-----+-------------------+\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
      "+----------------+----------------------+----------------------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assmble all feature columns into on single vector column\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['onehotencode_age', 'onehotencode_education', 'onehotencode_wantsMore'], outputCol='features')\n",
    "cuse_df_2 = assembler.transform(onehotencode_cuse).withColumnRenamed('y', 'label')\n",
    "cuse_df_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------------+-----+---------+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label| features|\n",
      "+----------------+----------------------+----------------------+-----+---------+\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
      "+----------------+----------------------+----------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Split data into training and testing\n",
    "train,test = cuse_df_2.randomSplit([0.8,0.2],seed=1234)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimatior\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "glm = GeneralizedLinearRegression(featuresCol='features',labelCol='label',family='binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameter grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "             addGrid(glm.regParam,[0,0.5,1,2,4]).\\\n",
    "             build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build cross-validation model\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=glm,estimatorParamMaps=param_grid,evaluator=evaluator,numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "## cv_model\n",
    "cv_model = cv.fit(cuse_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "pred_training_cv = cv_model.transform(train)\n",
    "pred_test_cv = cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label| features|        prediction|\n",
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_training_cv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label| features|        prediction|\n",
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151293|\n",
      "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test_cv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.2806, -0.7999, -1.1892, 0.325, -0.833])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056024275169240606"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6722119042705738"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(pred_training_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6769880972917027"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(pred_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneralizedLinearRegression_8803841886a1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
