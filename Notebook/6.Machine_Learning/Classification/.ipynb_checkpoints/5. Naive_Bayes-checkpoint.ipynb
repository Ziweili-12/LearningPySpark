{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = spark.read.csv(\"iris.csv\",header=True,inferSchema=True)\n",
    "iris.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sepal_length', 'double'),\n",
       " ('sepal_width', 'double'),\n",
       " ('petal_length', 'double'),\n",
       " ('petal_width', 'double'),\n",
       " ('species', 'string')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|summary|      sepal_length|        sepal_width|      petal_length|       petal_width|  species|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|  count|               150|                150|               150|               150|      150|\n",
      "|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|     null|\n",
      "| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|     null|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|   setosa|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|virginica|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge features to create a features-column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|         features|species|\n",
      "+-----------------+-------+\n",
      "|[5.1,3.5,1.4,0.2]| setosa|\n",
      "|[4.9,3.0,1.4,0.2]| setosa|\n",
      "|[4.7,3.2,1.3,0.2]| setosa|\n",
      "|[4.6,3.1,1.5,0.2]| setosa|\n",
      "|[5.0,3.6,1.4,0.2]| setosa|\n",
      "|[5.4,3.9,1.7,0.4]| setosa|\n",
      "|[4.6,3.4,1.4,0.3]| setosa|\n",
      "|[5.0,3.4,1.5,0.2]| setosa|\n",
      "|[4.4,2.9,1.4,0.2]| setosa|\n",
      "|[4.9,3.1,1.5,0.1]| setosa|\n",
      "|[5.4,3.7,1.5,0.2]| setosa|\n",
      "|[4.8,3.4,1.6,0.2]| setosa|\n",
      "|[4.8,3.0,1.4,0.1]| setosa|\n",
      "|[4.3,3.0,1.1,0.1]| setosa|\n",
      "|[5.8,4.0,1.2,0.2]| setosa|\n",
      "|[5.7,4.4,1.5,0.4]| setosa|\n",
      "|[5.4,3.9,1.3,0.4]| setosa|\n",
      "|[5.1,3.5,1.4,0.3]| setosa|\n",
      "|[5.7,3.8,1.7,0.3]| setosa|\n",
      "|[5.1,3.8,1.5,0.3]| setosa|\n",
      "+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris2 = iris.rdd.map(lambda x: Row(features = Vectors.dense(x[:-1]),species=x[-1])).toDF()\n",
    "iris2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index label column with StringIndexer\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringindexer = StringIndexer(inputCol=\"species\",outputCol='label')\n",
    "stages = [stringindexer]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-----+\n",
      "|         features|species|label|\n",
      "+-----------------+-------+-----+\n",
      "|[5.1,3.5,1.4,0.2]| setosa|  2.0|\n",
      "|[4.9,3.0,1.4,0.2]| setosa|  2.0|\n",
      "|[4.7,3.2,1.3,0.2]| setosa|  2.0|\n",
      "|[4.6,3.1,1.5,0.2]| setosa|  2.0|\n",
      "|[5.0,3.6,1.4,0.2]| setosa|  2.0|\n",
      "+-----------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df = pipeline.fit(iris2).transform(iris2)\n",
    "iris_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+\n",
      "|summary|  species|             label|\n",
      "+-------+---------+------------------+\n",
      "|  count|      150|               150|\n",
      "|   mean|     null|               1.0|\n",
      "| stddev|     null|0.8192319205190403|\n",
      "|    min|   setosa|               0.0|\n",
      "|    max|virginica|               2.0|\n",
      "+-------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('features', 'vector'), ('species', 'string'), ('label', 'double')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classification\n",
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = iris_df.randomSplit([0.8,0.2],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build cross-validation model\n",
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "naivebayes = NaiveBayes(featuresCol='features',labelCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "            addGrid(naivebayes.smoothing,[0,1,2,4,8]).\\\n",
    "            build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator\n",
    "There are three categories in the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build cross-validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "crossvalidator = CrossValidator(estimator=naivebayes,estimatorParamMaps=param_grid,evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit cross-validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation_mode = crossvalidator.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-----+--------------------+--------------------+----------+\n",
      "|         features|species|label|       rawPrediction|         probability|prediction|\n",
      "+-----------------+-------+-----+--------------------+--------------------+----------+\n",
      "|[4.4,3.2,1.3,0.2]| setosa|  2.0|[-12.291171378989...|[0.19555929123370...|       2.0|\n",
      "|[4.5,2.3,1.3,0.3]| setosa|  2.0|[-11.142786320680...|[0.26949095780384...|       2.0|\n",
      "|[4.6,3.1,1.5,0.2]| setosa|  2.0|[-12.550742246764...|[0.21656037289160...|       2.0|\n",
      "|[4.6,3.2,1.4,0.2]| setosa|  2.0|[-12.592365024690...|[0.20052392100539...|       2.0|\n",
      "|[4.6,3.4,1.4,0.3]| setosa|  2.0|[-13.148463542368...|[0.19824085251800...|       2.0|\n",
      "+-----------------+-------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_train = crossvalidation_mode.transform(train)\n",
    "pred_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-----+--------------------+--------------------+----------+\n",
      "|         features|species|label|       rawPrediction|         probability|prediction|\n",
      "+-----------------+-------+-----+--------------------+--------------------+----------+\n",
      "|[4.3,3.0,1.1,0.1]| setosa|  2.0|[-11.402228667810...|[0.18240945200010...|       2.0|\n",
      "|[4.4,2.9,1.4,0.2]| setosa|  2.0|[-11.923306551010...|[0.22553075812617...|       2.0|\n",
      "|[4.4,3.0,1.3,0.2]| setosa|  2.0|[-11.964929328937...|[0.20930932928504...|       2.0|\n",
      "|[4.8,3.1,1.6,0.2]| setosa|  2.0|[-12.851935892465...|[0.22180002940740...|       2.0|\n",
      "|[5.0,3.3,1.4,0.2]| setosa|  2.0|[-13.114876846919...|[0.18601727933390...|       2.0|\n",
      "+-----------------+-------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = crossvalidation_mode.transform(test)\n",
    "pred_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model from cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameter smoothing has best value: 8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The parameter smoothing has best value:\",crossvalidation_mode.bestModel._java_obj.getSmoothing())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Accuracy\n",
    "* f1\n",
    "* weightedPrecision\n",
    "* weightedRecall\n",
    "* accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction accuracy on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data(f1): 0.968236789665361\n",
      "training data(weightedPrecision): 0.9689250225835592\n",
      "training data(weightedRecall): 0.9682539682539681\n",
      "training data(accuracy): 0.9682539682539683\n"
     ]
    }
   ],
   "source": [
    "print(\"training data(f1):\",evaluator.setMetricName(\"f1\").evaluate(pred_train))\n",
    "print(\"training data(weightedPrecision):\",evaluator.setMetricName(\"weightedPrecision\").evaluate(pred_train))\n",
    "print(\"training data(weightedRecall):\",evaluator.setMetricName(\"weightedRecall\").evaluate(pred_train))\n",
    "print(\"training data(accuracy):\",evaluator.setMetricName(\"accuracy\").evaluate(pred_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data(f1): 0.958119658119658\n",
      "training data(weightedPrecision): 0.9635416666666667\n",
      "training data(weightedRecall): 0.9583333333333334\n",
      "training data(accuracy): 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"training data(f1):\",evaluator.setMetricName(\"f1\").evaluate(pred_test))\n",
    "print(\"training data(weightedPrecision):\",evaluator.setMetricName(\"weightedPrecision\").evaluate(pred_test))\n",
    "print(\"training data(weightedRecall):\",evaluator.setMetricName(\"weightedRecall\").evaluate(pred_test))\n",
    "print(\"training data(accuracy):\",evaluator.setMetricName(\"accuracy\").evaluate(pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {Row(label=2.0, prediction=2.0): 40,\n",
       "             Row(label=0.0, prediction=0.0): 42,\n",
       "             Row(label=1.0, prediction=1.0): 40,\n",
       "             Row(label=0.0, prediction=1.0): 1,\n",
       "             Row(label=1.0, prediction=0.0): 3})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training data\n",
    "train_conf_mat = pred_train.select(\"label\",\"prediction\")\n",
    "train_conf_mat.rdd.zipWithIndex().countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {Row(label=2.0, prediction=2.0): 10,\n",
       "             Row(label=0.0, prediction=0.0): 7,\n",
       "             Row(label=1.0, prediction=1.0): 6,\n",
       "             Row(label=1.0, prediction=0.0): 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing data\n",
    "test_conf_mat = pred_test.select(\"label\",'prediction')\n",
    "test_conf_mat.rdd.zipWithIndex().countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
